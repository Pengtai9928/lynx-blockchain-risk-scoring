{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600266114458",
   "display_name": "Python 3.8.5 64-bit ('lynx': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FastText Sentiment Analysis Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and pre-process data\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer # word lemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords # stopwords\n",
    "\n",
    "# format data\n",
    "from sklearn.model_selection import train_test_split # train test split\n",
    "import csv\n",
    "\n",
    "# modelling\n",
    "import io # to generate pre-trained vector\n",
    "import fasttext"
   ]
  },
  {
   "source": [
    "## Data Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text, lemmatize=True, stem=False):\n",
    "    # strip accents\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = str(text.decode(\"utf-8\"))\n",
    "\n",
    "    # covert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "\n",
    "    # remove unnecessary white spaces\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "\n",
    "    # tokenize\n",
    "    text_words = nltk.word_tokenize(text)\n",
    "\n",
    "    # lemmatize\n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        text_words = [wordnet_lemmatizer.lemmatize(x, pos=\"v\") for x in text_words]\n",
    "\n",
    "    # stem\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        text_words = [stemmer.stem(x) for x in text_words]\n",
    "\n",
    "    # remove stop words\n",
    "    stop = list(stopwords.words('english'))\n",
    "    keep_stopwords = [\"no\", \"not\", \"nor\"]\n",
    "    for word in keep_stopwords:\n",
    "        stop.remove(word)\n",
    "        stop = set(stop)\n",
    "    text_words = [x for x in text_words if not x in stop]\n",
    "\n",
    "    return ' '.join(text_words)"
   ]
  },
  {
   "source": [
    "### Conventional and Cryptonews Data\n",
    "#### Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "sample_crypto = pd.read_csv(\"data/sample_crypto.csv\", header=0)\n",
    "\n",
    "# combine title and excerpt\n",
    "sample_crypto[\"text\"] = sample_crypto[\"title\"].fillna('') + \" \" + sample_crypto[\"excerpt\"].fillna('')\n",
    "\n",
    "# sample text\n",
    "sample_crypto_text = sample_crypto[[\"title\", \"text\", \"label\"]]\n",
    "sample_crypto_excerpt = sample_crypto[[\"excerpt\", \"label\"]]\n",
    "sample_crypto_excerpt = sample_crypto_excerpt.dropna(subset=[\"excerpt\"])\n",
    "\n",
    "# sample with lemmatized words\n",
    "sample_crypto_lemmatize_text = sample_crypto_text.copy()\n",
    "sample_crypto_lemmatize_text[\"title\"] = sample_crypto_lemmatize_text[\"title\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "sample_crypto_lemmatize_text[\"text\"] = sample_crypto_lemmatize_text[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "sample_crypto_lemmatize_excerpt = sample_crypto_excerpt.copy()\n",
    "sample_crypto_lemmatize_excerpt[\"excerpt\"] = sample_crypto_lemmatize_excerpt[\"excerpt\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "\n",
    "# sample with stemmed words\n",
    "sample_crypto_stem_text = sample_crypto_text.copy()\n",
    "sample_crypto_stem_text[\"title\"] = sample_crypto_stem_text[\"title\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))\n",
    "sample_crypto_stem_text[\"text\"] = sample_crypto_stem_text[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "sample_crypto_stem_excerpt = sample_crypto_excerpt.copy()\n",
    "sample_crypto_stem_excerpt[\"excerpt\"] = sample_crypto_stem_excerpt[\"excerpt\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))\n",
    "\n",
    "# separate lemmatized X and y\n",
    "X_crypto_lemmatize_text = sample_crypto_lemmatize_text[[\"title\", \"text\"]]\n",
    "y_crypto_lemmatize_text = sample_crypto_lemmatize_text[\"label\"]\n",
    "X_crypto_lemmatize_excerpt = sample_crypto_lemmatize_excerpt[\"excerpt\"]\n",
    "y_crypto_lemmatize_excerpt = sample_crypto_lemmatize_excerpt[\"label\"]\n",
    "\n",
    "# separate stemmed X and y\n",
    "X_crypto_stem_text = sample_crypto_stem_text[[\"title\", \"text\"]]\n",
    "y_crypto_stem_text = sample_crypto_stem_text[\"label\"]\n",
    "X_crypto_stem_excerpt = sample_crypto_stem_excerpt[\"excerpt\"]\n",
    "y_crypto_stem_excerpt = sample_crypto_stem_excerpt[\"label\"]"
   ]
  },
  {
   "source": [
    "### Reddit Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reddit = pd.read_csv(\"data/sample_reddit.csv\", header=0)[[\"title\", \"excerpt\", \"label\"]]\n",
    "\n",
    "# combine title and excerpt (if any)\n",
    "sample_reddit[\"text\"] = sample_reddit[\"title\"].fillna('') + \" \" + sample_reddit[\"excerpt\"].fillna('')\n",
    "\n",
    "# sample with lemmatized words\n",
    "sample_reddit_lemmatize = sample_reddit.copy()\n",
    "sample_reddit_lemmatize[\"text\"] = sample_reddit_lemmatize[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "\n",
    "# sample with stemmed words\n",
    "sample_reddit_stem = sample_reddit.copy()\n",
    "sample_reddit_stem[\"text\"] = sample_reddit_stem[\"text\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))\n",
    "\n",
    "# separate lemmatized X and y\n",
    "X_reddit_lemmatize = sample_reddit_lemmatize[\"text\"]\n",
    "y_reddit_lemmatize = sample_reddit_lemmatize[\"label\"]\n",
    "\n",
    "# separate stemmed X and y\n",
    "X_reddit_stem = sample_reddit_stem[\"text\"]\n",
    "y_reddit_stem = sample_reddit_stem[\"label\"]"
   ]
  },
  {
   "source": [
    "### Twitter Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_twitter = pd.read_csv(\"data/sample_twitter.csv\", header=0)[[\"text\", \"label\"]]\n",
    "\n",
    "# remove any whitespaces in text\n",
    "sample_twitter[\"text\"] = sample_twitter[\"text\"].apply(lambda x: x.replace(\"\\n\",\"\"))\n",
    "\n",
    "# sample with lemmatized words\n",
    "sample_twitter_lemmatize = sample_twitter.copy()\n",
    "sample_twitter_lemmatize[\"text\"] = sample_twitter_lemmatize[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "\n",
    "# sample with stemmed words\n",
    "sample_twitter_stem = sample_twitter.copy()\n",
    "sample_twitter_stem[\"text\"] = sample_twitter_stem[\"text\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))\n",
    "\n",
    "# separate lemmatized X and y\n",
    "X_twitter_lemmatize = sample_twitter_lemmatize[\"text\"]\n",
    "y_twitter_lemmatize = sample_twitter_lemmatize[\"label\"]\n",
    "\n",
    "# separate stemmed X and y\n",
    "X_twitter_stem = sample_twitter_stem[\"text\"]\n",
    "y_twitter_stem = sample_twitter_stem[\"label\"]"
   ]
  },
  {
   "source": [
    "### Combined Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine lemmatized data\n",
    "sample_combined_lemmatize = pd.concat([sample_crypto_lemmatize_text[[\"text\", \"label\"]], sample_reddit_lemmatize[[\"text\", \"label\"]], sample_twitter_lemmatize])\n",
    "\n",
    "# combine stemmed data\n",
    "sample_combined_stem = pd.concat([sample_crypto_stem_text[[\"text\", \"label\"]], sample_reddit_stem[[\"text\", \"label\"]], sample_twitter_stem])\n",
    "\n",
    "# separate lemmatized X and y\n",
    "X_combined_lemmatize = sample_combined_lemmatize[\"text\"]\n",
    "y_combined_lemmatize = sample_combined_lemmatize[\"label\"]\n",
    "\n",
    "# separate stemmed X and y\n",
    "X_combined_stem = sample_combined_stem[\"text\"]\n",
    "y_combined_stem = sample_combined_stem[\"label\"]"
   ]
  },
  {
   "source": [
    "## Train-Test Split\n",
    "### Conventional and Cryptonews Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test-validation (lemmatize, title/text)\n",
    "X_crypto_lemmatize_text_train, X_crypto_lemmatize_text_test, y_crypto_lemmatize_text_train, y_crypto_lemmatize_text_test = train_test_split(X_crypto_lemmatize_text, y_crypto_lemmatize_text, test_size=0.2, random_state=123)\n",
    "X_crypto_lemmatize_text_train, X_crypto_lemmatize_text_val, y_crypto_lemmatize_text_train, y_crypto_lemmatize_text_val = train_test_split(X_crypto_lemmatize_text_train, y_crypto_lemmatize_text_train, test_size=0.25, random_state=123)\n",
    "X_crypto_lemmatize_title_train = X_crypto_lemmatize_text_train[\"title\"]\n",
    "X_crypto_lemmatize_text_train = X_crypto_lemmatize_text_train[\"text\"]\n",
    "\n",
    "# split train-test-validation (lemmatize, excerpt)\n",
    "X_crypto_lemmatize_excerpt_train, X_crypto_lemmatize_excerpt_test, y_crypto_lemmatize_excerpt_train, y_crypto_lemmatize_excerpt_test = train_test_split(X_crypto_lemmatize_excerpt, y_crypto_lemmatize_excerpt, test_size=0.2, random_state=123)\n",
    "X_crypto_lemmatize_excerpt_train, X_crypto_lemmatize_excerpt_val, y_crypto_lemmatize_excerpt_train, y_crypto_lemmatize_excerpt_val = train_test_split(X_crypto_lemmatize_excerpt_train, y_crypto_lemmatize_excerpt_train, test_size=0.25, random_state=123)\n",
    "\n",
    "# split train-test-validation (stem, title/text)\n",
    "X_crypto_stem_text_train, X_crypto_stem_text_test, y_crypto_stem_text_train, y_crypto_stem_text_test = train_test_split(X_crypto_stem_text, y_crypto_stem_text, test_size=0.2, random_state=123)\n",
    "X_crypto_stem_text_train, X_crypto_stem_text_val, y_crypto_stem_text_train, y_crypto_stem_text_val = train_test_split(X_crypto_stem_text_train, y_crypto_stem_text_train, test_size=0.25, random_state=123)\n",
    "X_crypto_stem_title_train = X_crypto_stem_text_train[\"title\"]\n",
    "X_crypto_stem_text_train = X_crypto_stem_text_train[\"text\"]\n",
    "\n",
    "# split train-test-validation (stem, excerpt)\n",
    "X_crypto_stem_excerpt_train, X_crypto_stem_excerpt_test, y_crypto_stem_excerpt_train, y_crypto_stem_excerpt_test = train_test_split(X_crypto_stem_excerpt, y_crypto_stem_excerpt, test_size=0.2, random_state=123)\n",
    "X_crypto_stem_excerpt_train, X_crypto_stem_excerpt_val, y_crypto_stem_excerpt_train, y_crypto_stem_excerpt_val = train_test_split(X_crypto_stem_excerpt_train, y_crypto_stem_excerpt_train, test_size=0.25, random_state=123)"
   ]
  },
  {
   "source": [
    "### Reddit Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test-validation (lemmatize)\n",
    "X_reddit_lemmatize_train, X_reddit_lemmatize_test, y_reddit_lemmatize_train, y_reddit_lemmatize_test = train_test_split(X_reddit_lemmatize, y_reddit_lemmatize, test_size=0.2, random_state=123)\n",
    "X_reddit_lemmatize_train, X_reddit_lemmatize_val, y_reddit_lemmatize_train, y_reddit_lemmatize_val = train_test_split(X_reddit_lemmatize_train, y_reddit_lemmatize_train, test_size=0.25, random_state=123)\n",
    "\n",
    "# split train-test-validation (stem)\n",
    "X_reddit_stem_train, X_reddit_stem_test, y_reddit_stem_train, y_reddit_stem_test = train_test_split(X_reddit_stem, y_reddit_stem, test_size=0.2, random_state=123)\n",
    "X_reddit_stem_train, X_reddit_stem_val, y_reddit_stem_train, y_reddit_stem_val = train_test_split(X_reddit_stem_train, y_reddit_stem_train, test_size=0.25, random_state=123)"
   ]
  },
  {
   "source": [
    "### Twitter Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test-validation (lemmatize)\n",
    "X_twitter_lemmatize_train, X_twitter_lemmatize_test, y_twitter_lemmatize_train, y_twitter_lemmatize_test = train_test_split(X_twitter_lemmatize, y_twitter_lemmatize, test_size=0.2, random_state=123)\n",
    "X_twitter_lemmatize_train, X_twitter_lemmatize_val, y_twitter_lemmatize_train, y_twitter_lemmatize_val = train_test_split(X_twitter_lemmatize_train, y_twitter_lemmatize_train, test_size=0.25, random_state=123)\n",
    "\n",
    "# split train-test-validation (stem)\n",
    "X_twitter_stem_train, X_twitter_stem_test, y_twitter_stem_train, y_twitter_stem_test = train_test_split(X_twitter_stem, y_twitter_stem, test_size=0.2, random_state=123)\n",
    "X_twitter_stem_train, X_twitter_stem_val, y_twitter_stem_train, y_twitter_stem_val = train_test_split(X_twitter_stem_train, y_twitter_stem_train, test_size=0.25, random_state=123)"
   ]
  },
  {
   "source": [
    "### Combined Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test-validation (lemmatize)\n",
    "X_combined_lemmatize_train, X_combined_lemmatize_test, y_combined_lemmatize_train, y_combined_lemmatize_test = train_test_split(X_combined_lemmatize, y_combined_lemmatize, test_size=0.2, random_state=123)\n",
    "X_combined_lemmatize_train, X_combined_lemmatize_val, y_combined_lemmatize_train, y_combined_lemmatize_val = train_test_split(X_combined_lemmatize_train, y_combined_lemmatize_train, test_size=0.25, random_state=123)\n",
    "\n",
    "# split train-test-validation (stem)\n",
    "X_combined_stem_train, X_combined_stem_test, y_combined_stem_train, y_combined_stem_test = train_test_split(X_combined_stem, y_combined_stem, test_size=0.2, random_state=123)\n",
    "X_combined_stem_train, X_combined_stem_val, y_combined_stem_train, y_combined_stem_val = train_test_split(X_combined_stem_train, y_combined_stem_train, test_size=0.25, random_state=123)"
   ]
  },
  {
   "source": [
    "## Formatting Data\n",
    "Prepare data in format required for fasttext model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(row, include_title=True, include_excerpt=False):\n",
    "    current_row = []\n",
    "\n",
    "    # prepare label\n",
    "    label = \"__label__\" + str(row[\"label\"])\n",
    "    current_row.append(label)\n",
    "\n",
    "    if include_title:\n",
    "        current_row.extend(nltk.word_tokenize(row[\"title\"]))\n",
    "    if include_excerpt:\n",
    "        current_row.extend(nltk.word_tokenize(row[\"excerpt\"]))\n",
    "    return current_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_format_data(data_list, filename_list):\n",
    "    for i in range(len(data_list)):\n",
    "        with open(labels[i], \"w\") as csvoutfile:\n",
    "            csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "            for row in sample_list[i]:\n",
    "                csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_lemmatize_title = sample_lemmatize.apply(lambda x: format_data(x, include_title=True, include_excerpt=False), axis=1)\n",
    "sample_lemmatize_excerpt = sample_lemmatize.apply(lambda x: format_data(x, include_title=False, include_excerpt=True), axis=1)\n",
    "sample_lemmatize_all = sample_lemmatize.apply(lambda x: format_data(x, include_title=True, include_excerpt=True), axis=1)\n",
    "\n",
    "sample_stem_title = sample_stem.apply(lambda x: format_data(x, include_title=True, include_excerpt=False), axis=1)\n",
    "sample_stem_excerpt = sample_stem.apply(lambda x: format_data(x, include_title=False, include_excerpt=True), axis=1)\n",
    "sample_stem_all = sample_stem.apply(lambda x: format_data(x, include_title=True, include_excerpt=True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data\n",
    "data_list = [sample_lemmatize_title, sample_lemmatize_excerpt, sample_lemmatize_all, sample_stem_title, sample_stem_excerpt, sample_stem_all]\n",
    "filename_list = [\"data/fasttext/sample_lemmatize_title.txt\", \"data/fasttext/sample_lemmatize_excerpt.txt\", \\\n",
    "    \"data/fasttext/sample_lemmatize_all.txt\", \"data/fasttext/sample_stem_title.txt\", \"data/fasttext/sample_stem_excerpt.txt\", \\\n",
    "        \"data/fasttext/sample_stem_all.txt\"]\n",
    "save_format_data(data_list=data_list, filename_list=filename_list)"
   ]
  },
  {
   "source": [
    "## Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('data/fasttext/sample_lemmatize_title.txt', dim=300, pretrained_vectors=\"utils/fasttext/wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(('__label__LOW',), array([1.00001001]))"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "\n",
    "model.predict('data/fasttext/sample_lemmatize_title.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sample_lemmatize_title' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-13526ad9aed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_lemmatize_title\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_lemmatize_title' is not defined"
     ]
    }
   ],
   "source": [
    "sample_lemmatize_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(('__label__HIGH',), array([0.99131745]))"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "model.predict(\"hack: person xx billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}