{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600841910527",
   "display_name": "Python 3.8.5 64-bit ('lynx': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FastText Sentiment Analysis Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and pre-process data\n",
    "import pandas as pd\n",
    "\n",
    "# modelling\n",
    "import fasttext\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate postfix (common to all train, test, validation sets)\n",
    "filename = [\"sample_crypto_lemmatize_title\", \"sample_crypto_lemmatize_excerpt\", \"sample_crypto_lemmatize_text\", \"sample_crypto_stem_title\", \"sample_crypto_stem_excerpt\", \"sample_crypto_stem_text\", \"sample_reddit_lemmatize\", \"sample_reddit_stem\", \"sample_twitter_lemmatize\", \"sample_twitter_stem\", \"sample_socialmedia_lemmatize\", \"sample_socialmedia_stem\", \"sample_all_lemmatize\", \"sample_all_stem\"]"
   ]
  },
  {
   "source": [
    "## Train and Test Model (Default Parameters)\n",
    "Perform initial testing to see which model performs best (before hyperparameter tuning)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve filenames\n",
    "train_all_filename_prefix = \"data/fasttext_date/train_all/\"\n",
    "train_all_filename_postfix = \".txt\"\n",
    "train_all_filename_list = [train_all_filename_prefix + filename[i] + train_all_filename_postfix for i in range(len(filename))]\n",
    "\n",
    "test_filename_prefix = \"data/fasttext_date/test/\"\n",
    "test_filename_postfix = \".csv\"\n",
    "test_filename_list = [test_filename_prefix + filename[i] + test_filename_postfix for i in range(len(filename))]\n",
    "\n",
    "# set model save dir\n",
    "model_filename_prefix = \"models/fasttext/default_date/\"\n",
    "model_filename_postfix = \".bin\"\n",
    "model_filename_list = [model_filename_prefix + filename[i] + model_filename_postfix for i in range(len(filename))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "metrics for: sample_crypto_lemmatize_title\nprecision score: 0.7254901960784313\nrecall score: 0.5522388059701493\nf1 score: 0.6271186440677965\n_____________________________________________________\nmetrics for: sample_crypto_lemmatize_excerpt\nprecision score: 0.5\nrecall score: 0.35\nf1 score: 0.4117647058823529\n_____________________________________________________\nmetrics for: sample_crypto_lemmatize_text\nprecision score: 0.6507936507936508\nrecall score: 0.6119402985074627\nf1 score: 0.6307692307692307\n_____________________________________________________\nmetrics for: sample_crypto_stem_title\nprecision score: 0.6481481481481481\nrecall score: 0.5223880597014925\nf1 score: 0.5785123966942148\n_____________________________________________________\nmetrics for: sample_crypto_stem_excerpt\nprecision score: 0.5454545454545454\nrecall score: 0.4\nf1 score: 0.4615384615384615\n_____________________________________________________\nmetrics for: sample_crypto_stem_text\nprecision score: 0.6721311475409836\nrecall score: 0.6119402985074627\nf1 score: 0.640625\n_____________________________________________________\nmetrics for: sample_reddit_lemmatize\nprecision score: 0.7933884297520661\nrecall score: 0.6713286713286714\nf1 score: 0.7272727272727274\n_____________________________________________________\nmetrics for: sample_reddit_stem\nprecision score: 0.7906976744186046\nrecall score: 0.7132867132867133\nf1 score: 0.7499999999999999\n_____________________________________________________\nmetrics for: sample_twitter_lemmatize\nprecision score: 0.8297872340425532\nrecall score: 0.9512195121951219\nf1 score: 0.8863636363636364\n_____________________________________________________\nmetrics for: sample_twitter_stem\nprecision score: 0.8297872340425532\nrecall score: 0.9512195121951219\nf1 score: 0.8863636363636364\n_____________________________________________________\nmetrics for: sample_socialmedia_lemmatize\nprecision score: 0.5853658536585366\nrecall score: 0.680161943319838\nf1 score: 0.6292134831460674\n_____________________________________________________\nmetrics for: sample_socialmedia_stem\nprecision score: 0.6153846153846154\nrecall score: 0.680161943319838\nf1 score: 0.6461538461538461\n_____________________________________________________\nmetrics for: sample_all_lemmatize\nprecision score: 0.649546827794562\nrecall score: 0.6107954545454546\nf1 score: 0.629575402635432\n_____________________________________________________\nmetrics for: sample_all_stem\nprecision score: 0.7009345794392523\nrecall score: 0.6392045454545454\nf1 score: 0.6686478454680534\n_____________________________________________________\n"
    }
   ],
   "source": [
    "# generate all models, output metrics and save model\n",
    "for i in range(len(train_all_filename_list)):\n",
    "    model = fasttext.train_supervised(train_all_filename_list[i], dim=300, pretrainedVectors=\"utils/fasttext/wiki-news-300d-1M.vec\")\n",
    "\n",
    "    test_df = pd.read_csv(test_filename_list[i], header=0)\n",
    "    y_test_pred = [int(model.predict(x)[0][0][-1]) for x in test_df[\"text\"]]\n",
    "    y_test_actual = list(test_df[\"label\"])\n",
    "    \n",
    "    # print metrics\n",
    "    print(\"metrics for:\", filename[i])\n",
    "    print(\"precision score:\", precision_score(y_true=y_test_actual, y_pred=y_test_pred, average=\"binary\", pos_label=1))\n",
    "    print(\"recall score:\", recall_score(y_true=y_test_actual, y_pred=y_test_pred, average=\"binary\", pos_label=1))\n",
    "    print(\"f1 score:\", f1_score(y_true=y_test_actual, y_pred=y_test_pred, average=\"binary\", pos_label=1))\n",
    "    print(\"_____________________________________________________\")\n",
    "    # save model\n",
    "    model.save_model(model_filename_list[i])"
   ]
  }
 ]
}