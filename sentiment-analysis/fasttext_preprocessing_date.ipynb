{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600832488402",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Fasttext Data Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and pre-process data\n",
    "import pandas as pd\n",
    "import string\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer # word lemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords # stopwords\n",
    "\n",
    "# format data\n",
    "import csv"
   ]
  },
  {
   "source": [
    "## Read and Clean Text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text, lemmatize=True, stem=False):\n",
    "    '''\n",
    "    Accepts a text and processes text\n",
    "    '''\n",
    "    # strip accents\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = str(text.decode(\"utf-8\"))\n",
    "\n",
    "    # covert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "\n",
    "    # remove unnecessary white spaces\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "\n",
    "    # tokenize\n",
    "    text_words = nltk.word_tokenize(text)\n",
    "\n",
    "    # lemmatize\n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        text_words = [wordnet_lemmatizer.lemmatize(x, pos=\"v\") for x in text_words]\n",
    "\n",
    "    # stem\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        text_words = [stemmer.stem(x) for x in text_words]\n",
    "\n",
    "    # remove stop words\n",
    "    stop = list(stopwords.words('english'))\n",
    "    keep_stopwords = [\"no\", \"not\", \"nor\"]\n",
    "    for word in keep_stopwords:\n",
    "        stop.remove(word)\n",
    "        stop = set(stop)\n",
    "    text_words = [x for x in text_words if not x in stop]\n",
    "\n",
    "    return ' '.join(text_words)"
   ]
  },
  {
   "source": [
    "### Conventional and Cryptonews Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "sample_crypto = pd.read_csv(\"data/sample_crypto.csv\", header=0)\n",
    "\n",
    "# combine title and excerpt\n",
    "sample_crypto[\"text\"] = sample_crypto[\"title\"].fillna('') + \" \" + sample_crypto[\"excerpt\"].fillna('')\n",
    "\n",
    "# sample text\n",
    "sample_crypto_text = sample_crypto[[\"date_time\", \"title\", \"text\", \"label\"]]\n",
    "sample_crypto_excerpt = sample_crypto[[\"date_time\", \"excerpt\", \"label\"]]\n",
    "sample_crypto_excerpt = sample_crypto_excerpt.dropna(subset=[\"excerpt\"])\n",
    "\n",
    "# sample with lemmatized words\n",
    "sample_crypto_lemmatize_text = sample_crypto_text.copy()\n",
    "sample_crypto_lemmatize_text[\"title\"] = sample_crypto_lemmatize_text[\"title\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "sample_crypto_lemmatize_text[\"text\"] = sample_crypto_lemmatize_text[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "sample_crypto_lemmatize_excerpt = sample_crypto_excerpt.copy()\n",
    "sample_crypto_lemmatize_excerpt[\"excerpt\"] = sample_crypto_lemmatize_excerpt[\"excerpt\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "\n",
    "# sample with stemmed words\n",
    "sample_crypto_stem_text = sample_crypto_text.copy()\n",
    "sample_crypto_stem_text[\"title\"] = sample_crypto_stem_text[\"title\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))\n",
    "sample_crypto_stem_text[\"text\"] = sample_crypto_stem_text[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "sample_crypto_stem_excerpt = sample_crypto_excerpt.copy()\n",
    "sample_crypto_stem_excerpt[\"excerpt\"] = sample_crypto_stem_excerpt[\"excerpt\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))"
   ]
  },
  {
   "source": [
    "### Reddit Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reddit = pd.read_csv(\"data/sample_reddit.csv\", header=0)[[\"date_time\", \"title\", \"excerpt\", \"label\"]]\n",
    "\n",
    "# combine title and excerpt (if any)\n",
    "sample_reddit[\"text\"] = sample_reddit[\"title\"].fillna('') + \" \" + sample_reddit[\"excerpt\"].fillna('')\n",
    "\n",
    "# sample with lemmatized words\n",
    "sample_reddit_lemmatize = sample_reddit.copy()\n",
    "sample_reddit_lemmatize[\"text\"] = sample_reddit_lemmatize[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "\n",
    "# sample with stemmed words\n",
    "sample_reddit_stem = sample_reddit.copy()\n",
    "sample_reddit_stem[\"text\"] = sample_reddit_stem[\"text\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))"
   ]
  },
  {
   "source": [
    "### Twitter Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_twitter = pd.read_csv(\"data/sample_twitter.csv\", header=0)[[\"date_time\", \"text\", \"label\"]]\n",
    "\n",
    "# remove any whitespaces in text\n",
    "sample_twitter[\"text\"] = sample_twitter[\"text\"].apply(lambda x: x.replace(\"\\n\",\"\"))\n",
    "\n",
    "# sample with lemmatized words\n",
    "sample_twitter_lemmatize = sample_twitter.copy()\n",
    "sample_twitter_lemmatize[\"text\"] = sample_twitter_lemmatize[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "\n",
    "# sample with stemmed words\n",
    "sample_twitter_stem = sample_twitter.copy()\n",
    "sample_twitter_stem[\"text\"] = sample_twitter_stem[\"text\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))"
   ]
  },
  {
   "source": [
    "### Combined Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# social media\n",
    "# combine lemmatized data\n",
    "sample_socialmedia_lemmatize = pd.concat([sample_reddit_lemmatize[[\"date_time\", \"text\", \"label\"]], sample_twitter_lemmatize])\n",
    "\n",
    "# combine stemmed data\n",
    "sample_socialmedia_stem = pd.concat([sample_reddit_stem[[\"date_time\", \"text\", \"label\"]], sample_twitter_stem])\n",
    "\n",
    "# all\n",
    "# combine lemmatized data\n",
    "sample_all_lemmatize = pd.concat([sample_crypto_lemmatize_text[[\"date_time\", \"text\", \"label\"]], sample_reddit_lemmatize[[\"date_time\", \"text\", \"label\"]], sample_twitter_lemmatize])\n",
    "\n",
    "# combine stemmed data\n",
    "sample_all_stem = pd.concat([sample_crypto_stem_text[[\"date_time\", \"text\", \"label\"]], sample_reddit_stem[[\"date_time\", \"text\", \"label\"]], sample_twitter_stem])"
   ]
  },
  {
   "source": [
    "## Train-Test-Validation Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_parsing_date(text):\n",
    "    for fmt in ('%Y-%m-%d %H:%M:%S', '%d/%m/%y %H:%M', '%d/%m/%y'):\n",
    "        try:\n",
    "            return datetime.strptime(text, fmt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        except TypeError:\n",
    "            return text\n",
    "        \n",
    "    raise ValueError('no valid date format found')\n",
    "\n",
    "def train_val_test_split(df):\n",
    "    '''\n",
    "    Output: train-set(80%), validation-set(20%), test set(20%)\n",
    "    '''\n",
    "    #Convert all date_time to datetime format\n",
    "    df['date_time'] = df['date_time'].apply(lambda x: try_parsing_date(x))\n",
    "    df = df[df['date_time'].notna()]\n",
    "\n",
    "    \n",
    "    #Sort df by date_time\n",
    "    df = df.sort_values('date_time', ascending = True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    #Num of rows in the dataframe\n",
    "    numrows = df.shape[0]\n",
    "\n",
    "    #Retrieve just the date \n",
    "    df['date'] =  df['date_time'].apply(lambda x: x.date())\n",
    "\n",
    "    #Get training dataset\n",
    "    #Get last date for the train set and get the data rows that fall within the date range\n",
    "    train_index = int(0.6*numrows) - 1\n",
    "    last_date1 = df.iloc[train_index]['date']\n",
    "    train = df[df['date'] <= last_date1]\n",
    "    \n",
    "    #Get validation dataset\n",
    "    num_rows_required = int(0.2*numrows) \n",
    "    val_index = train.shape[0]\n",
    "    last_date2 = df.iloc[val_index + num_rows_required - 1]['date']\n",
    "    mask = (df['date'] > last_date1) & (df['date'] <= last_date2)\n",
    "    val = df.loc[mask]\n",
    "\n",
    "    #Get testing dataset\n",
    "    test = df[df['date'] > last_date2]\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "source": [
    "### Conventional and Cryptonews Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (lemmatize, title/text)\n",
    "sample_crypto_lemmatize_text_train, sample_crypto_lemmatize_text_val, sample_crypto_lemmatize_text_test = train_val_test_split(sample_crypto_lemmatize_text)\n",
    "sample_crypto_lemmatize_text_train_all = pd.concat([sample_crypto_lemmatize_text_train, sample_crypto_lemmatize_text_val], axis=0)\n",
    "\n",
    "X_crypto_lemmatize_title_train_all = sample_crypto_lemmatize_text_train_all[\"title\"]\n",
    "X_crypto_lemmatize_text_train_all = sample_crypto_lemmatize_text_train_all[\"text\"]\n",
    "y_crypto_lemmatize_text_train_all = sample_crypto_lemmatize_text_train_all[\"label\"]\n",
    "\n",
    "X_crypto_lemmatize_title_train = sample_crypto_lemmatize_text_train[\"title\"]\n",
    "X_crypto_lemmatize_text_train = sample_crypto_lemmatize_text_train[\"text\"]\n",
    "y_crypto_lemmatize_text_train = sample_crypto_lemmatize_text_train[\"label\"]\n",
    "\n",
    "X_crypto_lemmatize_title_val = sample_crypto_lemmatize_text_val[\"title\"]\n",
    "X_crypto_lemmatize_text_val = sample_crypto_lemmatize_text_val[\"text\"]\n",
    "y_crypto_lemmatize_text_val = sample_crypto_lemmatize_text_val[\"label\"]\n",
    "\n",
    "X_crypto_lemmatize_title_test = sample_crypto_lemmatize_text_test[\"title\"]\n",
    "X_crypto_lemmatize_text_test = sample_crypto_lemmatize_text_test[\"text\"]\n",
    "y_crypto_lemmatize_text_test = sample_crypto_lemmatize_text_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (lemmatize, excerpt)\n",
    "sample_crypto_lemmatize_excerpt_train, sample_crypto_lemmatize_excerpt_val, sample_crypto_lemmatize_excerpt_test = train_val_test_split(sample_crypto_lemmatize_excerpt)\n",
    "sample_crypto_lemmatize_excerpt_train_all = pd.concat([sample_crypto_lemmatize_excerpt_train, sample_crypto_lemmatize_excerpt_val], axis=0)\n",
    "\n",
    "X_crypto_lemmatize_excerpt_train_all = sample_crypto_lemmatize_excerpt_train_all[\"excerpt\"]\n",
    "y_crypto_lemmatize_excerpt_train_all = sample_crypto_lemmatize_excerpt_train_all[\"label\"]\n",
    "\n",
    "X_crypto_lemmatize_excerpt_train = sample_crypto_lemmatize_excerpt_train[\"excerpt\"]\n",
    "y_crypto_lemmatize_excerpt_train = sample_crypto_lemmatize_excerpt_train[\"label\"]\n",
    "\n",
    "X_crypto_lemmatize_excerpt_val = sample_crypto_lemmatize_excerpt_val[\"excerpt\"]\n",
    "y_crypto_lemmatize_excerpt_val = sample_crypto_lemmatize_excerpt_val[\"label\"]\n",
    "\n",
    "X_crypto_lemmatize_excerpt_test = sample_crypto_lemmatize_excerpt_test[\"excerpt\"]\n",
    "y_crypto_lemmatize_excerpt_test = sample_crypto_lemmatize_excerpt_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (stem, title/text)\n",
    "sample_crypto_stem_text_train, sample_crypto_stem_text_val, sample_crypto_stem_text_test = train_val_test_split(sample_crypto_stem_text)\n",
    "sample_crypto_stem_text_train_all = pd.concat([sample_crypto_stem_text_train, sample_crypto_stem_text_val], axis=0)\n",
    "\n",
    "X_crypto_stem_title_train_all = sample_crypto_stem_text_train_all[\"title\"]\n",
    "X_crypto_stem_text_train_all = sample_crypto_stem_text_train_all[\"text\"]\n",
    "y_crypto_stem_text_train_all = sample_crypto_stem_text_train_all[\"label\"]\n",
    "\n",
    "X_crypto_stem_title_train = sample_crypto_stem_text_train[\"title\"]\n",
    "X_crypto_stem_text_train = sample_crypto_stem_text_train[\"text\"]\n",
    "y_crypto_stem_text_train = sample_crypto_stem_text_train[\"label\"]\n",
    "\n",
    "X_crypto_stem_title_val = sample_crypto_stem_text_val[\"title\"]\n",
    "X_crypto_stem_text_val = sample_crypto_stem_text_val[\"text\"]\n",
    "y_crypto_stem_text_val = sample_crypto_stem_text_val[\"label\"]\n",
    "\n",
    "X_crypto_stem_title_test = sample_crypto_stem_text_test[\"title\"]\n",
    "X_crypto_stem_text_test = sample_crypto_stem_text_test[\"text\"]\n",
    "y_crypto_stem_text_test = sample_crypto_stem_text_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (stem, excerpt)\n",
    "sample_crypto_stem_excerpt_train, sample_crypto_stem_excerpt_val, sample_crypto_stem_excerpt_test = train_val_test_split(sample_crypto_stem_excerpt)\n",
    "sample_crypto_stem_excerpt_train_all = pd.concat([sample_crypto_stem_excerpt_train, sample_crypto_stem_excerpt_val], axis=0)\n",
    "\n",
    "X_crypto_stem_excerpt_train_all = sample_crypto_stem_excerpt_train_all[\"excerpt\"]\n",
    "y_crypto_stem_excerpt_train_all = sample_crypto_stem_excerpt_train_all[\"label\"]\n",
    "\n",
    "X_crypto_stem_excerpt_train = sample_crypto_stem_excerpt_train[\"excerpt\"]\n",
    "y_crypto_stem_excerpt_train = sample_crypto_stem_excerpt_train[\"label\"]\n",
    "\n",
    "X_crypto_stem_excerpt_val = sample_crypto_stem_excerpt_val[\"excerpt\"]\n",
    "y_crypto_stem_excerpt_val = sample_crypto_stem_excerpt_val[\"label\"]\n",
    "\n",
    "X_crypto_stem_excerpt_test = sample_crypto_stem_excerpt_test[\"excerpt\"]\n",
    "y_crypto_stem_excerpt_test = sample_crypto_stem_excerpt_test[\"label\"]"
   ]
  },
  {
   "source": [
    "### Reddit Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (lemmatize)\n",
    "sample_reddit_lemmatize_train, sample_reddit_lemmatize_val, sample_reddit_lemmatize_test = train_val_test_split(sample_reddit_lemmatize)\n",
    "sample_reddit_lemmatize_train_all = pd.concat([sample_reddit_lemmatize_train, sample_reddit_lemmatize_val], axis=0)\n",
    "\n",
    "X_reddit_lemmatize_train_all = sample_reddit_lemmatize_train_all[\"text\"]\n",
    "y_reddit_lemmatize_train_all = sample_reddit_lemmatize_train_all[\"label\"]\n",
    "\n",
    "X_reddit_lemmatize_train = sample_reddit_lemmatize_train[\"text\"]\n",
    "y_reddit_lemmatize_train = sample_reddit_lemmatize_train[\"label\"]\n",
    "\n",
    "X_reddit_lemmatize_val = sample_reddit_lemmatize_val[\"text\"]\n",
    "y_reddit_lemmatize_val = sample_reddit_lemmatize_val[\"label\"]\n",
    "\n",
    "X_reddit_lemmatize_test = sample_reddit_lemmatize_test[\"text\"]\n",
    "y_reddit_lemmatize_test = sample_reddit_lemmatize_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (stem)\n",
    "sample_reddit_stem_train, sample_reddit_stem_val, sample_reddit_stem_test = train_val_test_split(sample_reddit_stem)\n",
    "sample_reddit_stem_train_all = pd.concat([sample_reddit_stem_train, sample_reddit_stem_val], axis=0)\n",
    "\n",
    "X_reddit_stem_train_all = sample_reddit_stem_train_all[\"text\"]\n",
    "y_reddit_stem_train_all = sample_reddit_stem_train_all[\"label\"]\n",
    "\n",
    "X_reddit_stem_train = sample_reddit_stem_train[\"text\"]\n",
    "y_reddit_stem_train = sample_reddit_stem_train[\"label\"]\n",
    "\n",
    "X_reddit_stem_val = sample_reddit_stem_val[\"text\"]\n",
    "y_reddit_stem_val = sample_reddit_stem_val[\"label\"]\n",
    "\n",
    "X_reddit_stem_test = sample_reddit_stem_test[\"text\"]\n",
    "y_reddit_stem_test = sample_reddit_stem_test[\"label\"]"
   ]
  },
  {
   "source": [
    "### Twitter Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (lemmatize)\n",
    "sample_twitter_lemmatize_train, sample_twitter_lemmatize_val, sample_twitter_lemmatize_test = train_val_test_split(sample_twitter_lemmatize)\n",
    "sample_twitter_lemmatize_train_all = pd.concat([sample_twitter_lemmatize_train, sample_twitter_lemmatize_val], axis=0)\n",
    "\n",
    "X_twitter_lemmatize_train_all = sample_twitter_lemmatize_train_all[\"text\"]\n",
    "y_twitter_lemmatize_train_all = sample_twitter_lemmatize_train_all[\"label\"]\n",
    "\n",
    "X_twitter_lemmatize_train = sample_twitter_lemmatize_train[\"text\"]\n",
    "y_twitter_lemmatize_train = sample_twitter_lemmatize_train[\"label\"]\n",
    "\n",
    "X_twitter_lemmatize_val = sample_twitter_lemmatize_val[\"text\"]\n",
    "y_twitter_lemmatize_val = sample_twitter_lemmatize_val[\"label\"]\n",
    "\n",
    "X_twitter_lemmatize_test = sample_twitter_lemmatize_test[\"text\"]\n",
    "y_twitter_lemmatize_test = sample_twitter_lemmatize_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (stem)\n",
    "sample_twitter_stem_train, sample_twitter_stem_val, sample_twitter_stem_test = train_val_test_split(sample_twitter_stem)\n",
    "sample_twitter_stem_train_all = pd.concat([sample_twitter_stem_train, sample_twitter_stem_val], axis=0)\n",
    "\n",
    "X_twitter_stem_train_all = sample_twitter_stem_train_all[\"text\"]\n",
    "y_twitter_stem_train_all = sample_twitter_stem_train_all[\"label\"]\n",
    "\n",
    "X_twitter_stem_train = sample_twitter_stem_train[\"text\"]\n",
    "y_twitter_stem_train = sample_twitter_stem_train[\"label\"]\n",
    "\n",
    "X_twitter_stem_val = sample_twitter_stem_val[\"text\"]\n",
    "y_twitter_stem_val = sample_twitter_stem_val[\"label\"]\n",
    "\n",
    "X_twitter_stem_test = sample_twitter_stem_test[\"text\"]\n",
    "y_twitter_stem_test = sample_twitter_stem_test[\"label\"]"
   ]
  },
  {
   "source": [
    "### Combined Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (social media, lemmatize)\n",
    "sample_socialmedia_lemmatize_train, sample_socialmedia_lemmatize_val, sample_socialmedia_lemmatize_test = train_val_test_split(sample_socialmedia_lemmatize)\n",
    "sample_socialmedia_lemmatize_train_all = pd.concat([sample_socialmedia_lemmatize_train, sample_socialmedia_lemmatize_val], axis=0)\n",
    "\n",
    "X_socialmedia_lemmatize_train_all = sample_socialmedia_lemmatize_train_all[\"text\"]\n",
    "y_socialmedia_lemmatize_train_all = sample_socialmedia_lemmatize_train_all[\"label\"]\n",
    "\n",
    "X_socialmedia_lemmatize_train = sample_socialmedia_lemmatize_train[\"text\"]\n",
    "y_socialmedia_lemmatize_train = sample_socialmedia_lemmatize_train[\"label\"]\n",
    "\n",
    "X_socialmedia_lemmatize_val = sample_socialmedia_lemmatize_val[\"text\"]\n",
    "y_socialmedia_lemmatize_val = sample_socialmedia_lemmatize_val[\"label\"]\n",
    "\n",
    "X_socialmedia_lemmatize_test = sample_socialmedia_lemmatize_test[\"text\"]\n",
    "y_socialmedia_lemmatize_test = sample_socialmedia_lemmatize_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (social media, stem)\n",
    "sample_socialmedia_stem_train, sample_socialmedia_stem_val, sample_socialmedia_stem_test = train_val_test_split(sample_socialmedia_stem)\n",
    "sample_socialmedia_stem_train_all = pd.concat([sample_socialmedia_stem_train, sample_socialmedia_stem_val], axis=0)\n",
    "\n",
    "X_socialmedia_stem_train_all = sample_socialmedia_stem_train_all[\"text\"]\n",
    "y_socialmedia_stem_train_all = sample_socialmedia_stem_train_all[\"label\"]\n",
    "\n",
    "X_socialmedia_stem_train = sample_socialmedia_stem_train[\"text\"]\n",
    "y_socialmedia_stem_train = sample_socialmedia_stem_train[\"label\"]\n",
    "\n",
    "X_socialmedia_stem_val = sample_socialmedia_stem_val[\"text\"]\n",
    "y_socialmedia_stem_val = sample_socialmedia_stem_val[\"label\"]\n",
    "\n",
    "X_socialmedia_stem_test = sample_socialmedia_stem_test[\"text\"]\n",
    "y_socialmedia_stem_test = sample_socialmedia_stem_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (all, lemmatize)\n",
    "sample_all_lemmatize_train, sample_all_lemmatize_val, sample_all_lemmatize_test = train_val_test_split(sample_all_lemmatize)\n",
    "sample_all_lemmatize_train_all = pd.concat([sample_all_lemmatize_train, sample_all_lemmatize_val], axis=0)\n",
    "\n",
    "X_all_lemmatize_train_all = sample_all_lemmatize_train_all[\"text\"]\n",
    "y_all_lemmatize_train_all = sample_all_lemmatize_train_all[\"label\"]\n",
    "\n",
    "X_all_lemmatize_train = sample_all_lemmatize_train[\"text\"]\n",
    "y_all_lemmatize_train = sample_all_lemmatize_train[\"label\"]\n",
    "\n",
    "X_all_lemmatize_val = sample_all_lemmatize_val[\"text\"]\n",
    "y_all_lemmatize_val = sample_all_lemmatize_val[\"label\"]\n",
    "\n",
    "X_all_lemmatize_test = sample_all_lemmatize_test[\"text\"]\n",
    "y_all_lemmatize_test = sample_all_lemmatize_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test (all, stem)\n",
    "sample_all_stem_train, sample_all_stem_val, sample_all_stem_test = train_val_test_split(sample_all_stem)\n",
    "sample_all_stem_train_all = pd.concat([sample_all_stem_train, sample_all_stem_val], axis=0)\n",
    "\n",
    "X_all_stem_train_all = sample_all_stem_train_all[\"text\"]\n",
    "y_all_stem_train_all = sample_all_stem_train_all[\"label\"]\n",
    "\n",
    "X_all_stem_train = sample_all_stem_train[\"text\"]\n",
    "y_all_stem_train = sample_all_stem_train[\"label\"]\n",
    "\n",
    "X_all_stem_val = sample_all_stem_val[\"text\"]\n",
    "y_all_stem_val = sample_all_stem_val[\"label\"]\n",
    "\n",
    "X_all_stem_test = sample_all_stem_test[\"text\"]\n",
    "y_all_stem_test = sample_all_stem_test[\"label\"]"
   ]
  },
  {
   "source": [
    "## Formatting and Saving Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(texts, labels):\n",
    "    '''\n",
    "    Accepts a series of texts and labels and outputs the formatted data for fasttext model\n",
    "    '''\n",
    "    formatted_data = []\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        current_row = []\n",
    "\n",
    "        # prepare label\n",
    "        current_row.append(\"__label__\" + str(list(labels)[i]))\n",
    "\n",
    "        # prepare text\n",
    "        current_row.extend(nltk.word_tokenize(list(texts)[i]))\n",
    "\n",
    "        # add to output\n",
    "        formatted_data.append(current_row)\n",
    "    \n",
    "    return pd.Series(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_data(text_list, label_list, filename_list):\n",
    "    '''\n",
    "    Accepts a list of texts, labels and filenames and saves the data into .txt file for each corresponding text, label and filename\n",
    "    '''\n",
    "    for i in range(len(filename_list)):\n",
    "        # format data\n",
    "        formatted_data = format_data(text_list[i], label_list[i])\n",
    "\n",
    "        # save data\n",
    "        filename = filename_list[i]\n",
    "        with open(filename, \"w\") as csvoutfile:\n",
    "            csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "            for row in formatted_data:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "def save_test_data(text_list, label_list, filename_list):\n",
    "    '''\n",
    "    Accepts a list of texts, labels and filenames and saves the data into a .csv file for reading later\n",
    "    '''\n",
    "    for i in range(len(filename_list)):\n",
    "        save_df = pd.concat([text_list[i], label_list[i]], axis=1)\n",
    "        save_df = save_df.reset_index(drop=True) # reset index\n",
    "        save_df.columns = [\"text\", \"label\"]\n",
    "        save_df.to_csv(filename_list[i], index=False) # save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate postfix (common to all train, test, validation sets)\n",
    "filename = [\"sample_crypto_lemmatize_title\", \"sample_crypto_lemmatize_excerpt\", \"sample_crypto_lemmatize_text\", \"sample_crypto_stem_title\", \"sample_crypto_stem_excerpt\", \"sample_crypto_stem_text\", \"sample_reddit_lemmatize\", \"sample_reddit_stem\", \"sample_twitter_lemmatize\", \"sample_twitter_stem\", \"sample_socialmedia_lemmatize\", \"sample_socialmedia_stem\", \"sample_all_lemmatize\", \"sample_all_stem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training (all) data as .txt for fasttext model input\n",
    "# set text and label data\n",
    "train_text_list_all = [X_crypto_lemmatize_title_train_all, X_crypto_lemmatize_excerpt_train_all, X_crypto_lemmatize_text_train_all, X_crypto_stem_title_train_all, X_crypto_stem_excerpt_train_all, X_crypto_stem_text_train_all, X_reddit_lemmatize_train_all, X_reddit_stem_train_all, X_twitter_lemmatize_train_all, X_twitter_stem_train_all, X_socialmedia_lemmatize_train_all, X_socialmedia_stem_train_all, X_all_lemmatize_train_all, X_all_stem_train_all]\n",
    "\n",
    "train_label_list_all = [y_crypto_lemmatize_text_train_all, y_crypto_lemmatize_excerpt_train_all, y_crypto_lemmatize_text_train_all, y_crypto_stem_text_train_all, y_crypto_stem_excerpt_train_all, y_crypto_stem_text_train_all, y_reddit_lemmatize_train_all, y_reddit_stem_train_all, y_twitter_lemmatize_train_all, y_twitter_stem_train_all, y_socialmedia_lemmatize_train_all, y_socialmedia_stem_train_all, y_all_lemmatize_train_all, y_all_stem_train_all]\n",
    "\n",
    "# set filenames\n",
    "train_all_filename_prefix = \"data/fasttext_date/train_all/\"\n",
    "train_all_filename_postfix = \".txt\"\n",
    "train_all_filename_list = [train_all_filename_prefix + filename[i] + train_all_filename_postfix for i in range(len(filename))]\n",
    "\n",
    "# save data\n",
    "save_train_data(text_list=train_text_list_all, label_list=train_label_list_all, filename_list=train_all_filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training data as .txt for fasttext model input\n",
    "# set text and label data\n",
    "train_text_list = [X_crypto_lemmatize_title_train, X_crypto_lemmatize_excerpt_train, X_crypto_lemmatize_text_train, X_crypto_stem_title_train, X_crypto_stem_excerpt_train, X_crypto_stem_text_train, X_reddit_lemmatize_train, X_reddit_stem_train, X_twitter_lemmatize_train, X_twitter_stem_train, X_socialmedia_lemmatize_train, X_socialmedia_stem_train, X_all_lemmatize_train, X_all_stem_train]\n",
    "\n",
    "train_label_list = [y_crypto_lemmatize_text_train, y_crypto_lemmatize_excerpt_train, y_crypto_lemmatize_text_train, y_crypto_stem_text_train, y_crypto_stem_excerpt_train, y_crypto_stem_text_train, y_reddit_lemmatize_train, y_reddit_stem_train, y_twitter_lemmatize_train, y_twitter_stem_train, y_socialmedia_lemmatize_train, y_socialmedia_stem_train, y_all_lemmatize_train, y_all_stem_train]\n",
    "\n",
    "# set filenames\n",
    "train_filename_prefix = \"data/fasttext_date/train/\"\n",
    "train_filename_postfix = \".txt\"\n",
    "train_filename_list = [train_filename_prefix + filename[i] + train_filename_postfix for i in range(len(filename))]\n",
    "\n",
    "# save data\n",
    "save_train_data(text_list=train_text_list, label_list=train_label_list, filename_list=train_filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save validation data as .csv\n",
    "# set text and label data\n",
    "validation_text_list = [X_crypto_lemmatize_title_val, X_crypto_lemmatize_excerpt_val, X_crypto_lemmatize_text_val, X_crypto_stem_title_val, X_crypto_stem_excerpt_val, X_crypto_stem_text_val, X_reddit_lemmatize_val, X_reddit_stem_val, X_twitter_lemmatize_val, X_twitter_stem_val, X_socialmedia_lemmatize_val, X_socialmedia_stem_val, X_all_lemmatize_val, X_all_stem_val]\n",
    "\n",
    "validation_label_list = [y_crypto_lemmatize_text_val, y_crypto_lemmatize_excerpt_val, y_crypto_lemmatize_text_val, y_crypto_stem_text_val, y_crypto_stem_excerpt_val, y_crypto_stem_text_val, y_reddit_lemmatize_test, y_reddit_stem_val, y_twitter_lemmatize_val, y_twitter_stem_val, y_socialmedia_lemmatize_val, y_socialmedia_stem_val, y_all_lemmatize_val, y_all_stem_val]\n",
    "\n",
    "# set filenames\n",
    "validation_filename_prefix = \"data/fasttext_date/validation/\"\n",
    "validation_filename_postfix = \".csv\"\n",
    "validation_filename_list = [validation_filename_prefix + filename[i] + validation_filename_postfix for i in range(len(filename))]\n",
    "\n",
    "\n",
    "save_test_data(text_list=validation_text_list, label_list=validation_label_list, filename_list=validation_filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save testing data as .csv\n",
    "# set text and label data\n",
    "test_text_list = [X_crypto_lemmatize_title_test, X_crypto_lemmatize_excerpt_test, X_crypto_lemmatize_text_test, X_crypto_stem_title_test, X_crypto_stem_excerpt_test, X_crypto_stem_text_test, X_reddit_lemmatize_test, X_reddit_stem_test, X_twitter_lemmatize_test, X_twitter_stem_test, X_socialmedia_lemmatize_test, X_socialmedia_stem_test, X_all_lemmatize_test, X_all_stem_test]\n",
    "\n",
    "test_label_list = [y_crypto_lemmatize_text_test, y_crypto_lemmatize_excerpt_test, y_crypto_lemmatize_text_test, y_crypto_stem_text_test, y_crypto_stem_excerpt_test, y_crypto_stem_text_test, y_reddit_lemmatize_test, y_reddit_stem_test, y_twitter_lemmatize_test, y_twitter_stem_test, y_socialmedia_lemmatize_test, y_socialmedia_stem_test, y_all_lemmatize_test, y_all_stem_test]\n",
    "\n",
    "# set filenames\n",
    "test_filename_prefix = \"data/fasttext_date/test/\"\n",
    "test_filename_postfix = \".csv\"\n",
    "test_filename_list = [test_filename_prefix + filename[i] + test_filename_postfix for i in range(len(filename))]\n",
    "\n",
    "save_test_data(text_list=test_text_list, label_list=test_label_list, filename_list=test_filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}